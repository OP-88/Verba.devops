#!/usr/bin/env python3
"""
Verba - Enhanced AI-Powered Meeting Assistant
Phase 1 Quality Enhancement Implementation
"""

import os
import sys
import json
import sqlite3
import asyncio
import logging
import tempfile
import threading
import time
import uuid
from contextlib import contextmanager
from datetime import datetime, timedelta
from pathlib import Path
from queue import Queue
from typing import Dict, List, Optional, Tuple, Any

import librosa
import numpy as np
import whisper
from fastapi import FastAPI, File, Form, HTTPException, UploadFile
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
import webrtcvad
import uvicorn
from scipy import signal
from sklearn.cluster import KMeans
from textstat import flesch_reading_ease, syllable_count
import requests

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ==================== ENHANCED AUDIO PREPROCESSING ====================

class AudioPreprocessor:
    """Advanced audio preprocessing pipeline for improved transcription quality"""
    
    def __init__(self):
        self.sample_rate = 16000
        self.hop_length = 512
        self.n_fft = 2048
        
    def normalize_audio(self, audio: np.ndarray) -> np.ndarray:
        """Normalize audio amplitude and remove DC offset"""
        # Remove DC offset
        audio = audio - np.mean(audio)
        
        # RMS normalization
        rms = np.sqrt(np.mean(audio ** 2))
        if rms > 0:
            audio = audio / rms * 0.1
            
        # Peak normalization (prevent clipping)
        peak = np.max(np.abs(audio))
        if peak > 0.95:
            audio = audio / peak * 0.95
            
        return audio
    
    def enhance_speech(self, audio: np.ndarray) -> np.ndarray:
        """Apply speech enhancement techniques"""
        # High-pass filter to remove low-frequency noise
        nyquist = self.sample_rate // 2
        low_cutoff = 80 / nyquist
        high_cutoff = 8000 / nyquist
        
        b, a = signal.butter(4, [low_cutoff, high_cutoff], btype='band')
        audio = signal.filtfilt(b, a, audio)
        
        # Dynamic range compression
        audio = np.sign(audio) * np.power(np.abs(audio), 0.7)
        
        return audio
    
    def reduce_noise(self, audio: np.ndarray) -> np.ndarray:
        """Simple spectral subtraction noise reduction"""
        # Get short-time fourier transform
        stft = librosa.stft(audio, n_fft=self.n_fft, hop_length=self.hop_length)
        magnitude, phase = np.abs(stft), np.angle(stft)
        
        # Estimate noise from first 0.5 seconds
        noise_frames = int(0.5 * self.sample_rate / self.hop_length)
        noise_spectrum = np.mean(magnitude[:, :noise_frames], axis=1, keepdims=True)
        
        # Spectral subtraction
        alpha = 2.0  # Over-subtraction factor
        beta = 0.01  # Spectral floor
        
        enhanced_magnitude = magnitude - alpha * noise_spectrum
        enhanced_magnitude = np.maximum(enhanced_magnitude, beta * magnitude)
        
        # Reconstruct audio
        enhanced_stft = enhanced_magnitude * np.exp(1j * phase)
        enhanced_audio = librosa.istft(enhanced_stft, hop_length=self.hop_length)
        
        return enhanced_audio
    
    def preprocess(self, audio: np.ndarray, content_type: str = "speech") -> np.ndarray:
        """Complete preprocessing pipeline"""
        logger.info(f"🔧 Preprocessing audio (type: {content_type})")
        
        # Basic normalization
        audio = self.normalize_audio(audio)
        
        if content_type == "speech":
            # Apply speech-specific enhancements
            audio = self.enhance_speech(audio)
            audio = self.reduce_noise(audio)
        elif content_type == "music":
            # For music, lighter processing to preserve quality
            audio = self.enhance_speech(audio)  # Still helps with vocals
            
        # Final normalization
        audio = self.normalize_audio(audio)
        
        logger.info("✅ Audio preprocessing complete")
        return audio

# ==================== CONTENT TYPE DETECTION ====================

class ContentTypeDetector:
    """Intelligent content type detection for context-aware transcription"""
    
    def __init__(self):
        self.sample_rate = 16000
        
    def analyze_audio_features(self, audio: np.ndarray) -> Dict[str, float]:
        """Extract audio features for content classification"""
        # Spectral features
        stft = librosa.stft(audio)
        spectral_centroids = librosa.feature.spectral_centroid(S=np.abs(stft), sr=self.sample_rate)[0]
        spectral_rolloff = librosa.feature.spectral_rolloff(S=np.abs(stft), sr=self.sample_rate)[0]
        
        # Rhythm features
        tempo, beats = librosa.beat.beat_track(y=audio, sr=self.sample_rate)
        
        # Energy features
        rms_energy = librosa.feature.rms(y=audio)[0]
        
        # Zero crossing rate
        zcr = librosa.feature.zero_crossing_rate(audio)[0]
        
        return {
            'spectral_centroid_mean': np.mean(spectral_centroids),
            'spectral_centroid_std': np.std(spectral_centroids),
            'spectral_rolloff_mean': np.mean(spectral_rolloff),
            'tempo': tempo,
            'rms_energy_mean': np.mean(rms_energy),
            'rms_energy_std': np.std(rms_energy),
            'zcr_mean': np.mean(zcr),
            'beat_strength': len(beats) / (len(audio) / self.sample_rate) if len(beats) > 0 else 0
        }
    
    def detect_content_type(self, audio: np.ndarray) -> Tuple[str, float]:
        """Detect if audio is speech, music, or mixed content"""
        features = self.analyze_audio_features(audio)
        
        # Rule-based classification
        music_score = 0.0
        speech_score = 0.0
        
        # Music indicators
        if features['tempo'] > 60 and features['tempo'] < 180:
            music_score += 0.3
        if features['beat_strength'] > 0.5:
            music_score += 0.3
        if features['spectral_centroid_std'] > 800:
            music_score += 0.2
        if features['rms_energy_std'] > 0.02:
            music_score += 0.2
            
        # Speech indicators
        if features['spectral_centroid_mean'] < 2000:
            speech_score += 0.3
        if features['zcr_mean'] < 0.1:
            speech_score += 0.2
        if features['rms_energy_mean'] > 0.01 and features['rms_energy_mean'] < 0.05:
            speech_score += 0.3
        if features['spectral_centroid_std'] < 500:
            speech_score += 0.2
            
        # Determine content type
        if music_score > speech_score + 0.1:
            return "music", music_score
        elif speech_score > music_score + 0.1:
            return "speech", speech_score
        else:
            return "mixed", max(music_score, speech_score)

# ==================== CONTEXT-AWARE PROMPTING ====================

class ContextAwarePrompting:
    """Generate intelligent prompts based on content type and context"""
    
    def __init__(self):
        self.prompts = {
            "speech": {
                "meeting": "Business meeting, professional discussion, clear speech",
                "lecture": "Educational content, academic presentation, technical terms",
                "conversation": "Natural conversation, informal dialogue, everyday speech",
                "interview": "Interview format, questions and answers, professional tone"
            },
            "music": {
                "rap": "Rap music, hip-hop lyrics, rhythmic speech, slang terms",
                "song": "Song lyrics, musical performance, clear vocal pronunciation",
                "spoken_word": "Poetry, spoken word performance, artistic expression"
            },
            "mixed": {
                "presentation": "Presentation with music, mixed audio content",
                "podcast": "Podcast with intro music, varied content types",
                "video": "Video content with background music and speech"
            }
        }
    
    def get_whisper_prompt(self, content_type: str, context: str = None) -> str:
        """Generate context-aware prompt for Whisper"""
        if context and content_type in self.prompts and context in self.prompts[content_type]:
            return self.prompts[content_type][context]
        elif content_type in self.prompts:
            # Return default prompt for content type
            return list(self.prompts[content_type].values())[0]
        else:
            return "Clear audio transcription, accurate speech recognition"
    
    def detect_context(self, audio: np.ndarray, content_type: str) -> str:
        """Detect specific context within content type"""
        # Simple heuristic-based context detection
        audio_duration = len(audio) / 16000
        
        if content_type == "speech":
            if audio_duration > 1800:  # > 30 minutes
                return "lecture"
            elif audio_duration > 300:  # > 5 minutes
                return "meeting"
            else:
                return "conversation"
                
        elif content_type == "music":
            # Analyze for rap characteristics
            tempo_features = librosa.feature.tempogram(y=audio, sr=16000)
            if np.mean(tempo_features) > 0.5:  # High rhythmic content
                return "rap"
            else:
                return "song"
                
        else:  # mixed
            return "podcast"

# ==================== ACCURACY BENCHMARKING ====================

class AccuracyBenchmarker:
    """Benchmark and evaluate transcription accuracy"""
    
    def __init__(self):
        self.metrics = {}
        
    def analyze_transcription_quality(self, text: str) -> Dict[str, float]:
        """Analyze transcription quality metrics"""
        if not text.strip():
            return {"error": "Empty transcription"}
            
        # Readability metrics
        flesch_score = flesch_reading_ease(text)
        
        # Text statistics
        words = text.split()
        sentences = text.split('.')
        
        # Calculate metrics
        avg_word_length = np.mean([len(word) for word in words]) if words else 0
        avg_sentence_length = np.mean([len(sentence.split()) for sentence in sentences if sentence.strip()]) if sentences else 0
        
        # Repetition detection
        word_freq = {}
        for word in words:
            word_lower = word.lower().strip('.,!?')
            word_freq[word_lower] = word_freq.get(word_lower, 0) + 1
            
        repetition_ratio = sum(1 for count in word_freq.values() if count > 1) / len(word_freq) if word_freq else 0
        
        return {
            "flesch_score": flesch_score,
            "word_count": len(words),
            "sentence_count": len([s for s in sentences if s.strip()]),
            "avg_word_length": avg_word_length,
            "avg_sentence_length": avg_sentence_length,
            "repetition_ratio": repetition_ratio,
            "unique_words": len(word_freq),
            "vocabulary_richness": len(word_freq) / len(words) if words else 0
        }
    
    def benchmark_transcription(self, audio_file: str, transcription: str, 
                              content_type: str, processing_time: float) -> Dict[str, Any]:
        """Generate comprehensive benchmark report"""
        quality_metrics = self.analyze_transcription_quality(transcription)
        
        # Audio metrics
        audio_duration = librosa.get_duration(filename=audio_file) if os.path.exists(audio_file) else 0
        
        benchmark = {
            "timestamp": datetime.now().isoformat(),
            "content_type": content_type,
            "audio_duration": audio_duration,
            "processing_time": processing_time,
            "real_time_factor": processing_time / audio_duration if audio_duration > 0 else 0,
            "transcription_length": len(transcription),
            "quality_metrics": quality_metrics,
            "performance_rating": self._calculate_performance_rating(quality_metrics, processing_time, audio_duration)
        }
        
        return benchmark
    
    def _calculate_performance_rating(self, quality_metrics: Dict, processing_time: float, audio_duration: float) -> str:
        """Calculate overall performance rating"""
        score = 0
        
        # Quality factors
        if quality_metrics.get("vocabulary_richness", 0) > 0.5:
            score += 2
        if quality_metrics.get("avg_word_length", 0) > 3:
            score += 1
        if quality_metrics.get("repetition_ratio", 1) < 0.3:
            score += 2
            
        # Performance factors
        if audio_duration > 0:
            rtf = processing_time / audio_duration
            if rtf < 0.5:
                score += 3
            elif rtf < 1.0:
                score += 2
            elif rtf < 2.0:
                score += 1
                
        # Rating scale
        if score >= 7:
            return "Excellent"
        elif score >= 5:
            return "Good"
        elif score >= 3:
            return "Fair"
        else:
            return "Needs Improvement"

# ==================== ENHANCED DATABASE MANAGER ====================

class EnhancedDatabaseManager:
    """Enhanced database management with connection pooling and performance optimization"""
    
    def __init__(self, db_path: str, pool_size: int = 5):
        self.db_path = db_path
        self.pool_size = pool_size
        self.connection_pool = Queue(maxsize=pool_size)
        self.pool_lock = threading.Lock()
        self._initialize_pool()
        self._create_tables()
        
    def _initialize_pool(self):
        """Initialize database connection pool"""
        for _ in range(self.pool_size):
            conn = sqlite3.connect(
                self.db_path,
                timeout=30,
                check_same_thread=False
            )
            conn.execute("PRAGMA journal_mode=WAL")
            conn.execute("PRAGMA synchronous=NORMAL")
            conn.execute("PRAGMA cache_size=10000")
            conn.execute("PRAGMA temp_store=MEMORY")
            self.connection_pool.put(conn)
        
        logger.info(f"✅ Enhanced Database initialized: {self.db_path} (pool size: {self.pool_size})")
    
    @contextmanager
    def get_connection(self):
        """Get database connection from pool"""
        conn = None
        try:
            conn = self.connection_pool.get(timeout=10)
            yield conn
        except Exception as e:
            if conn:
                conn.rollback()
            raise e
        finally:
            if conn:
                self.connection_pool.put(conn)
    
    def _create_tables(self):
        """Create enhanced database schema"""
        with self.get_connection() as conn:
            # Enhanced transcriptions table
            conn.execute("""
                CREATE TABLE IF NOT EXISTS transcriptions (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    session_id TEXT NOT NULL,
                    filename TEXT,
                    transcription TEXT NOT NULL,
                    content_type TEXT DEFAULT 'speech',
                    context TEXT,
                    processing_time REAL,
                    audio_duration REAL,
                    quality_score REAL,
                    benchmark_data TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # Enhanced performance tracking
            conn.execute("""
                CREATE TABLE IF NOT EXISTS performance_metrics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    session_id TEXT NOT NULL,
                    transcription_id INTEGER,
                    content_type TEXT,
                    processing_time REAL,
                    audio_duration REAL,
                    real_time_factor REAL,
                    quality_rating TEXT,
                    accuracy_score REAL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (transcription_id) REFERENCES transcriptions (id)
                )
            """)
            
            # Keep existing tables for compatibility
            conn.execute("""
                CREATE TABLE IF NOT EXISTS sessions (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    session_id TEXT UNIQUE NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            conn.execute("""
                CREATE TABLE IF NOT EXISTS reminders (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    session_id TEXT NOT NULL,
                    title TEXT NOT NULL,
                    description TEXT,
                    due_date DATE,
                    status TEXT DEFAULT 'pending',
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            conn.execute("""
                CREATE TABLE IF NOT EXISTS usage_stats (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    session_id TEXT,
                    action TEXT NOT NULL,
                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    metadata TEXT
                )
            """)
            
            conn.execute("""
                CREATE TABLE IF NOT EXISTS settings (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    category TEXT NOT NULL,
                    key TEXT NOT NULL,
                    value TEXT NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(category, key)
                )
            """)
            
            conn.commit()

# ==================== ENHANCED TRANSCRIPTION SERVICE ====================

class EnhancedTranscriptionService:
    """Enhanced transcription service with quality improvements"""
    
    def __init__(self, model_size: str = "base"):  # Upgraded from tiny
        self.model_size = model_size
        self.model = None
        self.vad = webrtcvad.Vad(2)
        self.preprocessor = AudioPreprocessor()
        self.content_detector = ContentTypeDetector()
        self.prompt_generator = ContextAwarePrompting()
        self.benchmarker = AccuracyBenchmarker()
        self._load_model()
        
    def _load_model(self):
        """Load Whisper model with enhanced configuration"""
        logger.info(f"🤖 Loading Whisper {self.model_size} model...")
        start_time = time.time()
        self.model = whisper.load_model(self.model_size)
        load_time = time.time() - start_time
        logger.info(f"✅ Model loaded in {load_time:.2f} seconds")
        
    def detect_voice_activity(self, audio: np.ndarray, sample_rate: int = 16000) -> List[Tuple[float, float]]:
        """Enhanced voice activity detection"""
        # Convert to bytes for VAD
        audio_int16 = (audio * 32768).astype(np.int16)
        frame_duration = 30  # ms
        frame_size = int(sample_rate * frame_duration / 1000)
        
        voiced_segments = []
        current_segment_start = None
        
        for i in range(0, len(audio_int16) - frame_size, frame_size):
            frame = audio_int16[i:i + frame_size].tobytes()
            
            try:
                is_speech = self.vad.is_speech(frame, sample_rate)
                timestamp = i / sample_rate
                
                if is_speech and current_segment_start is None:
                    current_segment_start = timestamp
                elif not is_speech and current_segment_start is not None:
                    voiced_segments.append((current_segment_start, timestamp))
                    current_segment_start = None
            except:
                continue
                
        # Handle case where audio ends with speech
        if current_segment_start is not None:
            voiced_segments.append((current_segment_start, len(audio_int16) / sample_rate))
            
        # Merge close segments and filter short ones
        merged_segments = []
        for start, end in voiced_segments:
            if end - start >= 0.5:  # Minimum 0.5 seconds
                if merged_segments and start - merged_segments[-1][1] < 1.0:  # Merge if gap < 1 second
                    merged_segments[-1] = (merged_segments[-1][0], end)
                else:
                    merged_segments.append((start, end))
                    
        logger.info(f"✅ Detected {len(merged_segments)} voice segments")
        return merged_segments
    
    def transcribe_audio(self, audio_path: str, meeting_title: str = "") -> Dict[str, Any]:
        """Enhanced transcription with quality improvements"""
        start_time = time.time()
        
        try:
            # Load and analyze audio
            logger.info(f"🎵 Loading audio: {audio_path}")
            audio, sample_rate = librosa.load(audio_path, sr=16000)
            
            # Detect content type
            content_type, confidence = self.content_detector.detect_content_type(audio)
            context = self.prompt_generator.detect_context(audio, content_type)
            
            logger.info(f"🔍 Detected content type: {content_type} ({confidence:.2f} confidence)")
            logger.info(f"📝 Context: {context}")
            
            # Preprocess audio
            enhanced_audio = self.preprocessor.preprocess(audio, content_type)
            
            # Voice activity detection
            logger.info("🔍 Detecting voice segments...")
            voiced_segments = self.detect_voice_activity(enhanced_audio)
            
            if not voiced_segments:
                return {
                    "transcription": "No speech detected in audio",
                    "content_type": content_type,
                    "context": context,
                    "processing_time": time.time() - start_time,
                    "segments": []
                }
            
            # Generate context-aware prompt
            prompt = self.prompt_generator.get_whisper_prompt(content_type, context)
            logger.info(f"🎯 Using prompt: {prompt}")
            
            # Transcribe segments
            full_transcription = []
            segment_details = []
            
            for i, (start, end) in enumerate(voiced_segments):
                logger.info(f"🎙️ Transcribing segment {i+1}/{len(voiced_segments)}")
                
                # Extract segment
                start_sample = int(start * 16000)
                end_sample = int(end * 16000)
                segment_audio = enhanced_audio[start_sample:end_sample]
                
                # Transcribe with context-aware prompt
                result = self.model.transcribe(
                    segment_audio,
                    initial_prompt=prompt,
                    word_timestamps=True,
                    temperature=0.2,  # Lower temperature for more consistent results
                    best_of=3,        # Multiple attempts for better accuracy
                    beam_size=5       # Beam search for better results
                )
                
                segment_text = result["text"].strip()
                if segment_text:
                    full_transcription.append(segment_text)
                    segment_details.append({
                        "start": start,
                        "end": end,
                        "text": segment_text,
                        "confidence": getattr(result, 'confidence', 0.0)
                    })
            
            final_transcription = " ".join(full_transcription)
            processing_time = time.time() - start_time
            
            logger.info(f"✅ Transcription completed in {processing_time:.2f}s")
            
            # Benchmark results
            benchmark = self.benchmarker.benchmark_transcription(
                audio_path, final_transcription, content_type, processing_time
            )
            
            return {
                "transcription": final_transcription,
                "content_type": content_type,
                "context": context,
                "processing_time": processing_time,
                "segments": segment_details,
                "benchmark": benchmark,
                "quality_rating": benchmark["performance_rating"]
            }
            
        except Exception as e:
            logger.error(f"❌ Transcription failed: {str(e)}")
            return {
                "error": str(e),
                "transcription": "",
                "processing_time": time.time() - start_time
            }

# ==================== FASTAPI APPLICATION ====================

# Initialize services
db_manager = EnhancedDatabaseManager("verba_app.db")
transcription_service = EnhancedTranscriptionService("base")  # Upgraded model

app = FastAPI(title="Verba Enhanced API", version="1.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Pydantic models
class TranscriptionResponse(BaseModel):
    transcription: str
    content_type: str
    context: str
    processing_time: float
    quality_rating: str
    segments: List[Dict]
    benchmark: Dict

class ReminderCreate(BaseModel):
    title: str
    description: str = ""
    due_date: Optional[str] = None

class ReminderUpdate(BaseModel):
    title: Optional[str] = None
    description: Optional[str] = None
    due_date: Optional[str] = None
    status: Optional[str] = None

# ==================== API ENDPOINTS ====================

@app.get("/")
async def root():
    return {"message": "Verba Enhanced API", "version": "1.0.0", "status": "operational"}

@app.get("/health")
async def health_check():
    """Enhanced health check with system status"""
    try:
        # Test database
        with db_manager.get_connection() as conn:
            conn.execute("SELECT 1").fetchone()
        
        # Test model
        model_status = "loaded" if transcription_service.model else "not_loaded"
        
        return {
            "status": "healthy",
            "database": "operational",
            "model": model_status,
            "model_size": transcription_service.model_size,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}

@app.post("/transcribe", response_model=TranscriptionResponse)
async def transcribe_audio(
    audio: UploadFile = File(...),
    session_id: str = Form(...),
    meeting_title: str = Form("")
):
    """Enhanced transcription endpoint with quality improvements"""
    if not session_id:
        raise HTTPException(status_code=400, detail="Session ID is required")
        
    # Save uploaded file
    with tempfile.NamedTemporaryFile(delete=False, suffix=Path(audio.filename).suffix) as tmp_file:
        content = await audio.read()
        tmp_file.write(content)
        tmp_file.flush()
        
        try:
            # Transcribe with enhancements
            result = transcription_service.transcribe_audio(tmp_file.name, meeting_title)
            
            if "error" in result:
                raise HTTPException(status_code=500, detail=result["error"])
            
            # Save to database with enhanced data
            with db_manager.get_connection() as conn:
                cursor = conn.execute("""
                    INSERT INTO transcriptions (
                        session_id, filename, transcription, content_type, context,
                        processing_time, audio_duration, quality_score, benchmark_data
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    session_id,
                    audio.filename,
                    result["transcription"],
                    result["content_type"],
                    result["context"],
                    result["processing_time"],
                    result.get("benchmark", {}).get("audio_duration", 0),
                    result.get("benchmark", {}).get("quality_metrics", {}).get("vocabulary_richness", 0),
                    json.dumps(result.get("benchmark", {}))
                ))
                
                transcription_id = cursor.lastrowid
                
                # Save performance metrics
                conn.execute("""
                    INSERT INTO performance_metrics (
                        session_id, transcription_id, content_type, processing_time,
                        audio_duration, real_time_factor, quality_rating, accuracy_score
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    session_id,
                    transcription_id,
                    result["content_type"],
                    result["processing_time"],
                    result.get("benchmark", {}).get("audio_duration", 0),
                    result.get("benchmark", {}).get("real_time_factor", 0),
                    result["quality_rating"],
                    result.get("benchmark", {}).get("quality_metrics", {}).get("vocabulary_richness", 0) * 100
                ))
                
                conn.commit()
            
            return TranscriptionResponse(**result)
            
        finally:
            # Clean up temp file
            os.unlink(tmp_file.name)

@app.get("/transcriptions")
async def get_transcriptions(session_id: str, limit: int = 20, offset: int = 0):
    """Get transcriptions with enhanced metadata"""
    with db_manager.get_connection() as conn:
        cursor = conn.execute("""
            SELECT 
                t.id, t.filename, t.transcription, t.content_type, t.context,
                t.processing_time, t.audio_duration, t.quality_score, t.created_at,
                pm.quality_rating, pm.real_time_factor
            FROM transcriptions t
            LEFT JOIN performance_metrics pm ON t.id = pm.transcription_id
            WHERE t.session_id = ?
            ORDER BY t.created_at DESC
            LIMIT ? OFFSET ?
        """, (session_id, limit, offset))
        
        results = cursor.fetchall()
        
        return [{
            "id": row[0],
            "filename": row[1],
            "transcription": row[2],
            "content_type": row[3],
            "context": row[4],
            "processing_time": row[5],
            "audio_duration": row[6],
            "quality_score": row[7],
            "created_at": row[8],
            "quality_rating": row[9],
            "real_time_factor": row[10]
        } for row in results]

@app.get("/analytics")
async def get_analytics(session_id: str):
    """Get enhanced analytics and performance metrics"""
    with db_manager.get_connection() as conn:
        # Overall stats
        stats = conn.execute("""
            SELECT 
                COUNT(*) as total_transcriptions,
                AVG(processing_time) as avg_processing_time,
                AVG(audio_duration) as avg_audio_duration,
                AVG(real_time_factor) as avg_real_time_factor,
                AVG(accuracy_score) as avg_accuracy_score
            FROM performance_metrics
            WHERE session_id = ?
        """, (session_id,)).fetchone()
        
        # Content type breakdown
        content_types = conn.execute("""
            SELECT content_type, COUNT(*) as count, AVG(accuracy_score) as avg_accuracy
            FROM performance_metrics
            WHERE session_id = ?
            GROUP BY content_type
        """, (session_id,)).fetchall()
        
        # Quality ratings
        quality_ratings = conn.execute("""
            SELECT quality_rating, COUNT(*) as count
            FROM performance_metrics
            WHERE session_id = ?
            GROUP BY quality_rating
        """, (session_id,)).fetchall()
        
        # Recent performance trend
        recent_performance = conn.execute("""
            SELECT 
                DATE(created_at) as date,
                AVG(accuracy_score) as avg_accuracy,
                AVG(real_time_factor) as avg_speed,
                COUNT(*) as transcriptions
            FROM performance_metrics
            WHERE session_id = ? AND created_at >= date('now', '-7 days')
            GROUP BY DATE(created_at)
            ORDER BY date
        """, (session_id,)).fetchall()
        
        return {
            "overall_stats": {
                "total_transcriptions": stats[0] or 0,
                "avg_processing_time": round(stats[1] or 0, 2),
                "avg_audio_duration": round(stats[2] or 0, 2),
                "avg_real_time_factor": round(stats[3] or 0, 2),
                "avg_accuracy_score": round(stats[4] or 0, 2)
            },
            "content_type_breakdown": [
                {
                    "type": row[0],
                    "count": row[1],
                    "avg_accuracy": round(row[2] or 0, 2)
                } for row in content_types
            ],
            "quality_distribution": [
                {
                    "rating": row[0],
                    "count": row[1]
                } for row in quality_ratings
            ],
            "performance_trend": [
                {
                    "date": row[0],
                    "avg_accuracy": round(row[1] or 0, 2),
                    "avg_speed": round(row[2] or 0, 2),
                    "transcriptions": row[3]
                } for row in recent_performance
            ]
        }

@app.get("/sessions/{session_id}")
async def get_session_info(session_id: str):
    """Get detailed session information with performance metrics"""
    with db_manager.get_connection() as conn:
        # Session basic info
        session = conn.execute("""
            SELECT session_id, created_at FROM sessions WHERE session_id = ?
        """, (session_id,)).fetchone()
        
        if not session:
            raise HTTPException(status_code=404, detail="Session not found")
        
        # Performance summary
        performance = conn.execute("""
            SELECT 
                COUNT(*) as total_transcriptions,
                SUM(audio_duration) as total_audio_duration,
                SUM(processing_time) as total_processing_time,
                AVG(accuracy_score) as avg_accuracy,
                MAX(quality_rating) as best_quality,
                MIN(quality_rating) as worst_quality
            FROM performance_metrics
            WHERE session_id = ?
        """, (session_id,)).fetchone()
        
        # Content type summary
        content_summary = conn.execute("""
            SELECT content_type, COUNT(*) as count
            FROM performance_metrics
            WHERE session_id = ?
            GROUP BY content_type
        """, (session_id,)).fetchall()
        
        return {
            "session_id": session[0],
            "created_at": session[1],
            "performance_summary": {
                "total_transcriptions": performance[0] or 0,
                "total_audio_duration": round(performance[1] or 0, 2),
                "total_processing_time": round(performance[2] or 0, 2),
                "avg_accuracy": round(performance[3] or 0, 2),
                "best_quality": performance[4],
                "worst_quality": performance[5]
            },
            "content_distribution": [
                {"type": row[0], "count": row[1]} for row in content_summary
            ]
        }

# ==================== EXISTING ENDPOINTS (Enhanced) ====================

@app.post("/sessions")
async def create_session():
    """Create a new session with enhanced tracking"""
    session_id = f"session_{uuid.uuid4().hex[:8]}_{int(time.time() * 1000)}"
    
    with db_manager.get_connection() as conn:
        conn.execute("""
            INSERT INTO sessions (session_id) VALUES (?)
        """, (session_id,))
        
        # Log session creation
        conn.execute("""
            INSERT INTO usage_stats (session_id, action, metadata)
            VALUES (?, ?, ?)
        """, (session_id, "session_created", json.dumps({"timestamp": datetime.now().isoformat()})))
        
        conn.commit()
    
    logger.info(f"✅ Session created: {session_id}")
    return {"session_id": session_id}

@app.post("/reminders")
async def create_reminder(session_id: str, reminder: ReminderCreate):
    """Create a new reminder"""
    with db_manager.get_connection() as conn:
        cursor = conn.execute("""
            INSERT INTO reminders (session_id, title, description, due_date)
            VALUES (?, ?, ?, ?)
        """, (session_id, reminder.title, reminder.description, reminder.due_date))
        
        reminder_id = cursor.lastrowid
        conn.commit()
    
    return {"id": reminder_id, "message": "Reminder created successfully"}

@app.get("/reminders")
async def get_reminders(session_id: str):
    """Get all reminders for a session"""
    with db_manager.get_connection() as conn:
        cursor = conn.execute("""
            SELECT id, title, description, due_date, status, created_at, updated_at
            FROM reminders
            WHERE session_id = ?
            ORDER BY created_at DESC
        """, (session_id,))
        
        return [
            {
                "id": row[0],
                "title": row[1],
                "description": row[2],
                "due_date": row[3],
                "status": row[4],
                "created_at": row[5],
                "updated_at": row[6]
            }
            for row in cursor.fetchall()
        ]

@app.put("/reminders/{reminder_id}")
async def update_reminder(
    reminder_id: int,
    session_id: str,
    status: Optional[str] = None,
    reminder_update: Optional[ReminderUpdate] = None
):
    """Update reminder status or details"""
    updates = []
    params = []
    
    if status:
        updates.append("status = ?")
        params.append(status)
    
    if reminder_update:
        if reminder_update.title:
            updates.append("title = ?")
            params.append(reminder_update.title)
        if reminder_update.description is not None:
            updates.append("description = ?")
            params.append(reminder_update.description)
        if reminder_update.due_date:
            updates.append("due_date = ?")
            params.append(reminder_update.due_date)
        if reminder_update.status:
            updates.append("status = ?")
            params.append(reminder_update.status)
    
    if not updates:
        raise HTTPException(status_code=400, detail="No updates provided")
    
    updates.append("updated_at = CURRENT_TIMESTAMP")
    params.extend([reminder_id, session_id])
    
    with db_manager.get_connection() as conn:
        result = conn.execute(f"""
            UPDATE reminders 
            SET {', '.join(updates)}
            WHERE id = ? AND session_id = ?
        """, params)
        
        if result.rowcount == 0:
            raise HTTPException(status_code=404, detail="Reminder not found")
        
        conn.commit()
    
    return {"message": "Reminder updated successfully"}

@app.delete("/reminders/{reminder_id}")
async def delete_reminder(reminder_id: int, session_id: str):
    """Delete a reminder"""
    with db_manager.get_connection() as conn:
        result = conn.execute("""
            DELETE FROM reminders WHERE id = ? AND session_id = ?
        """, (reminder_id, session_id))
        
        if result.rowcount == 0:
            raise HTTPException(status_code=404, detail="Reminder not found")
        
        conn.commit()
    
    return {"message": "Reminder deleted successfully"}

@app.get("/export/{session_id}")
async def export_session_data(session_id: str, format: str = "json"):
    """Export session data in various formats"""
    with db_manager.get_connection() as conn:
        # Get all transcriptions
        transcriptions = conn.execute("""
            SELECT 
                t.id, t.filename, t.transcription, t.content_type, t.context,
                t.processing_time, t.audio_duration, t.quality_score, t.created_at,
                pm.quality_rating, pm.real_time_factor, pm.accuracy_score
            FROM transcriptions t
            LEFT JOIN performance_metrics pm ON t.id = pm.transcription_id
            WHERE t.session_id = ?
            ORDER BY t.created_at
        """, (session_id,)).fetchall()
        
        # Get reminders
        reminders = conn.execute("""
            SELECT id, title, description, due_date, status, created_at
            FROM reminders WHERE session_id = ?
        """, (session_id,)).fetchall()
        
        export_data = {
            "session_id": session_id,
            "exported_at": datetime.now().isoformat(),
            "transcriptions": [
                {
                    "id": row[0],
                    "filename": row[1],
                    "transcription": row[2],
                    "content_type": row[3],
                    "context": row[4],
                    "processing_time": row[5],
                    "audio_duration": row[6],
                    "quality_score": row[7],
                    "created_at": row[8],
                    "quality_rating": row[9],
                    "real_time_factor": row[10],
                    "accuracy_score": row[11]
                } for row in transcriptions
            ],
            "reminders": [
                {
                    "id": row[0],
                    "title": row[1],
                    "description": row[2],
                    "due_date": row[3],
                    "status": row[4],
                    "created_at": row[5]
                } for row in reminders
            ]
        }
    
    if format.lower() == "json":
        return export_data
    else:
        raise HTTPException(status_code=400, detail="Only JSON format supported currently")

# ==================== AI ASSISTANT INTEGRATION ====================

class AIAssistant:
    """Enhanced AI assistant with better context awareness"""
    
    def __init__(self, api_key: str = None):
        self.api_key = api_key or os.getenv("OPENROUTER_API_KEY")
        self.base_url = "https://openrouter.ai/api/v1/chat/completions"
        self.available = bool(self.api_key)
        
        if self.available:
            logger.info("✅ Enhanced AI Assistant initialized")
        else:
            logger.warning("⚠️ AI Assistant not available: OPENROUTER_API_KEY environment variable not set")
    
    def analyze_transcription(self, transcription: str, content_type: str, context: str) -> str:
        """Enhanced transcription analysis with content-aware prompts"""
        if not self.available:
            return "AI analysis not available. Please set OPENROUTER_API_KEY environment variable."
        
        # Content-aware system prompts
        system_prompts = {
            "speech": "You are an expert meeting analyst. Provide concise summaries, key points, and action items from spoken content.",
            "music": "You are a music content analyst. Analyze lyrics, themes, and provide insights about musical content while being respectful of artistic expression.",
            "mixed": "You are a multimedia content analyst. Analyze mixed audio content including both speech and music elements."
        }
        
        system_prompt = system_prompts.get(content_type, system_prompts["speech"])
        
        # Context-specific instructions
        context_instructions = {
            "meeting": "Focus on decisions made, action items, and key discussion points.",
            "lecture": "Identify main concepts, learning objectives, and important details.",
            "conversation": "Summarize key topics and sentiment of the discussion.",
            "rap": "Analyze lyrical themes, wordplay, and artistic expression respectfully.",
            "song": "Discuss lyrical meaning, emotional themes, and artistic elements.",
            "interview": "Highlight key questions, answers, and insights shared."
        }
        
        context_instruction = context_instructions.get(context, "Provide a comprehensive analysis.")
        
        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": "anthropic/claude-3.5-sonnet",
                "messages": [
                    {
                        "role": "system",
                        "content": f"{system_prompt} {context_instruction}"
                    },
                    {
                        "role": "user",
                        "content": f"Please analyze this {content_type} content ({context}):\n\n{transcription}"
                    }
                ],
                "max_tokens": 500,
                "temperature": 0.3
            }
            
            response = requests.post(self.base_url, headers=headers, json=data, timeout=30)
            
            if response.status_code == 200:
                return response.json()["choices"][0]["message"]["content"]
            else:
                logger.error(f"AI API Error: {response.status_code} - {response.text}")
                return f"AI analysis failed: {response.status_code}"
                
        except Exception as e:
            logger.error(f"AI Assistant error: {str(e)}")
            return f"AI analysis error: {str(e)}"

# Initialize AI assistant
ai_assistant = AIAssistant()

@app.post("/analyze")
async def analyze_transcription(
    session_id: str,
    transcription_id: int,
    custom_prompt: Optional[str] = None
):
    """Analyze transcription with enhanced AI capabilities"""
    with db_manager.get_connection() as conn:
        result = conn.execute("""
            SELECT transcription, content_type, context 
            FROM transcriptions 
            WHERE id = ? AND session_id = ?
        """, (transcription_id, session_id)).fetchone()
        
        if not result:
            raise HTTPException(status_code=404, detail="Transcription not found")
        
        transcription, content_type, context = result
        
        if custom_prompt:
            # Use custom analysis prompt
            analysis = ai_assistant.analyze_transcription(
                f"Custom analysis request: {custom_prompt}\n\nContent: {transcription}",
                content_type,
                context
            )
        else:
            # Use standard content-aware analysis
            analysis = ai_assistant.analyze_transcription(transcription, content_type, context)
        
        # Store analysis result
        conn.execute("""
            UPDATE transcriptions 
            SET updated_at = CURRENT_TIMESTAMP 
            WHERE id = ?
        """, (transcription_id,))
        
        conn.commit()
    
    return {
        "analysis": analysis,
        "content_type": content_type,
        "context": context
    }

@app.post("/chat")
async def chat_with_ai(session_id: str, message: str):
    """Enhanced chat with session context"""
    if not ai_assistant.available:
        raise HTTPException(status_code=503, detail="AI Assistant not available")
    
    # Get recent transcriptions for context
    with db_manager.get_connection() as conn:
        recent_transcriptions = conn.execute("""
            SELECT transcription, content_type, context, created_at
            FROM transcriptions 
            WHERE session_id = ?
            ORDER BY created_at DESC
            LIMIT 3
        """, (session_id,)).fetchall()
        
        context_info = []
        for trans in recent_transcriptions:
            context_info.append(f"[{trans[1]}:{trans[2]}] {trans[0][:200]}...")
    
    context_prompt = "Recent session context:\n" + "\n".join(context_info) if context_info else "No recent context available."
    
    try:
        headers = {
            "Authorization": f"Bearer {ai_assistant.api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": "anthropic/claude-3.5-sonnet",
            "messages": [
                {
                    "role": "system",
                    "content": f"You are Verba, an AI assistant specialized in helping with transcription analysis and meeting insights. {context_prompt}"
                },
                {
                    "role": "user",
                    "content": message
                }
            ],
            "max_tokens": 400,
            "temperature": 0.7
        }
        
        response = requests.post(ai_assistant.base_url, headers=headers, json=data, timeout=30)
        
        if response.status_code == 200:
            ai_response = response.json()["choices"][0]["message"]["content"]
            
            # Log chat interaction
            with db_manager.get_connection() as conn:
                conn.execute("""
                    INSERT INTO usage_stats (session_id, action, metadata)
                    VALUES (?, ?, ?)
                """, (session_id, "ai_chat", json.dumps({
                    "user_message": message[:100],
                    "ai_response_length": len(ai_response)
                })))
                conn.commit()
            
            return {"response": ai_response}
        else:
            raise HTTPException(status_code=response.status_code, detail="AI service error")
            
    except Exception as e:
        logger.error(f"Chat error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# ==================== APPLICATION LIFECYCLE ====================

@app.on_event("startup")
async def startup_event():
    """Enhanced startup with system checks"""
    logger.info("🚀 Starting Verba Enhanced Backend...")
    
    # Verify model is loaded
    if not transcription_service.model:
        logger.error("❌ Whisper model not loaded!")
        sys.exit(1)
    
    # Verify database
    try:
        with db_manager.get_connection() as conn:
            conn.execute("SELECT 1").fetchone()
        logger.info("✅ Database connection verified")
    except Exception as e:
        logger.error(f"❌ Database error: {e}")
        sys.exit(1)
    
    logger.info("✅ Verba Enhanced Backend started successfully!")

@app.on_event("shutdown")
async def shutdown_event():
    """Enhanced cleanup on shutdown"""
    logger.info("🛑 Shutting down Verba Enhanced Backend...")
    
    # Clean up connection pool
    try:
        while not db_manager.connection_pool.empty():
            conn = db_manager.connection_pool.get_nowait()
            conn.close()
        logger.info("✅ Database connections cleaned up")
    except Exception as e:
        logger.error(f"⚠️ Cleanup warning: {e}")
    
    logger.info("✅ Verba Enhanced Backend shutdown complete")

# ==================== TESTING SUITE ====================

def run_enhanced_tests():
    """Comprehensive testing suite for enhanced features"""
    print("🧪 Testing Enhanced Verba Integration...")
    
    # Test database with enhanced features
    test_db = EnhancedDatabaseManager("test_verba_enhanced.db")
    
    try:
        # Test session creation
        session_id = f"test_session_{int(time.time())}"
        with test_db.get_connection() as conn:
            conn.execute("INSERT INTO sessions (session_id) VALUES (?)", (session_id,))
            conn.commit()
        print(f"✅ Enhanced session created: {session_id}")
        
        # Test content type detection
        detector = ContentTypeDetector()
        # Create test audio (440Hz sine wave)
        test_audio = np.sin(2 * np.pi * 440 * np.linspace(0, 1, 16000))
        content_type, confidence = detector.detect_content_type(test_audio)
        print(f"✅ Content detection: {content_type} ({confidence:.2f})")
        
        # Test preprocessing
        preprocessor = AudioPreprocessor()
        enhanced_audio = preprocessor.preprocess(test_audio, content_type)
        print(f"✅ Audio preprocessing: {len(enhanced_audio)} samples")
        
        # Test prompting
        prompt_gen = ContextAwarePrompting()
        prompt = prompt_gen.get_whisper_prompt(content_type, "meeting")
        print(f"✅ Context prompt: {prompt[:50]}...")
        
        # Test benchmarking
        benchmarker = AccuracyBenchmarker()
        quality = benchmarker.analyze_transcription_quality("This is a test transcription with various words.")
        print(f"✅ Quality analysis: {quality['vocabulary_richness']:.2f} richness")
        
        # Test enhanced transcription storage
        with test_db.get_connection() as conn:
            conn.execute("""
                INSERT INTO transcriptions (
                    session_id, filename, transcription, content_type, context,
                    processing_time, audio_duration, quality_score
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (session_id, "test.wav", "Test transcription", content_type, "test", 1.5, 2.0, 0.85))
            
            # Test performance metrics
            conn.execute("""
                INSERT INTO performance_metrics (
                    session_id, content_type, processing_time, audio_duration,
                    real_time_factor, quality_rating, accuracy_score
                ) VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (session_id, content_type, 1.5, 2.0, 0.75, "Good", 85.0))
            
            conn.commit()
        print("✅ Enhanced database storage")
        
        # Test analytics
        with test_db.get_connection() as conn:
            stats = conn.execute("""
                SELECT COUNT(*), AVG(accuracy_score) FROM performance_metrics WHERE session_id = ?
            """, (session_id,)).fetchone()
        print(f"✅ Analytics: {stats[0]} transcriptions, {stats[1]:.1f}% avg accuracy")
        
        print("🎉 Enhanced integration testing complete!")
        print("✅ All enhanced features working correctly!")
        
        return True
        
    except Exception as e:
        print(f"❌ Enhanced test failed: {e}")
        return False
        
    finally:
        # Cleanup
        try:
            while not test_db.connection_pool.empty():
                conn = test_db.connection_pool.get_nowait()
                conn.close()
            os.unlink("test_verba_enhanced.db")
            print("✅ Test database cleaned up")
        except:
            pass

# ==================== MAIN EXECUTION ====================

if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        run_enhanced_tests()
    elif len(sys.argv) > 1 and sys.argv[1] == "server":
        logger.info("🚀 Starting Verba Enhanced Server...")
        uvicorn.run(app, host="0.0.0.0", port=8000)
    else:
        print("Usage:")
        print("  python main.py test    - Run enhanced testing suite")
        print("  python main.py server  - Start enhanced API server")
